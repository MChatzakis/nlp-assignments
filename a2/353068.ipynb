{"cells":[{"cell_type":"markdown","metadata":{"id":"P0Iyg6btLW9M"},"source":["#  Assignment 2 - Transfer Learning and Data Augmentation 💬\n","\n","Welcome to the **second assignment** for the **CS-552: Modern NLP course**!\n","\n","> - 😀 Name: **Emmanouil Chatzakis**\n","> - ✉️ Email: **emmanouil.chatzakis@epfl.ch**\n","> - 🪪 SCIPER: **353068**"]},{"cell_type":"markdown","metadata":{"id":"_XjnQhbFIJUu"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;\">\n","\n","## **Assignment Description**\n","- In the first part of this assignment, you will need to implement training (fine-tuning) and evaluation of a pre-trained language model ([DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) ), on natural language inference (NLI) task for recognizing textual entailment (RTE).\n","\n","- Following the first finetuning task, you will need to identify the shortcut (i.e. some salient or toxic features) that the model learnt for the specific task. \n","\n","- For part-3, you are supposed to annotate 100 randomly assigned test datapoints as ground-truth labels. Additionally, the cross annotation should be conducted by another one or two annotators, and you will learn about how to calculate the agreement statistics as a significant characteristic reflecting the quality of a collected dataset.\n","\n","- For part-4, since the human annotation is quite time- and effort-consuming, there are plenty of ways to get silver-labels from automatic labeling to augment the dataset scale. We provide the reference to some simple methods (EDA and Back Translation) but you are encouraged to explore other advanced mechanisms. You will evaluate the improvement of your model performance by using your data augmentation method.\n","\n","For each part, you will need to complete the code in the corresponding `.py` files (`nli.py` for Part-1, `shortcut.py` for Part-2, `eda.py` for Part-4). You will be provided with the function descriptions and detailed instructions about the code snippet you need to write.\n","\n","\n","### Table of Contents\n","- **[PART 1: Model Finetuning for NLI](#1)**\n","    - [1.1 Data Processing](#11)\n","    - [1.2 Model Training and Evaluation](#12)\n","- **[PART 2: Identify Model Shortcut](#2)**\n","    - [2.1 Word-Pair Pattern Extraction](#21)\n","    - [2.2 Distill Potentially Useful Patterns](#22)\n","    - [2.3 Case Study](#23)\n","- **[PART 3: Annotate New Data](#3)**\n","    - [3.1 Write an Annotation Guideline](#31)\n","    - [3.2 Annotate Your 100 Datapoints with Partner(s)](#32)\n","    - [3.3 Agreement Measure](#33)\n","    - [3.4 Robustness Check](#34)\n","- **[PART 4: Data Augmentation](#4)**\n","    \n","### Deliverables\n","\n","- ✅ This jupyter notebook\n","- ✅ `nli.py` file\n","- ✅ `shortcut.py` file\n","- ✅ Finetuned DistilBERT models for NLI task (Part 1 and Part 4)\n","- ✅ Annotated and cross-annotated data files (Part 3)\n","- ✅ New dataset from data augmentation (Part 4)\n","\n","</div>"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":353,"status":"ok","timestamp":1680881895451,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"sGvNvjalnQUr"},"outputs":[],"source":["USE_COLAB = True"]},{"cell_type":"markdown","metadata":{"id":"lluaZwaS-0v9"},"source":["### Google Colab Setup\n","If you are using Google Colab notebook for this assignment, you will need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the popped window, sign in to your Google account. (The same account you used to store this notebook!)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2483,"status":"ok","timestamp":1680881898996,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"VfVHqiSvK1aB","outputId":"c8dbc8a4-2b5c-40da-9f46-2d4b3af9c24c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["if USE_COLAB:\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/drive\")\n"]},{"cell_type":"markdown","metadata":{"id":"opCteyIv-_kS"},"source":["Now first click the 4th left-side bar (named Files), then click the 2nd bar popped under Files column (named Refresh), under \"/drive/MyDrive/\" find the Assignment 2 folder that you uploaded to your Google Drive, copy its path and fill it in below. If everything is working correctly, then running the folowing cell should print the filenames from the assignment:\n","\n","```\n","['Assignment2.ipynb', 'requirements.txt', 'runs', 'predictions', 'nli_data', 'testA2.py', 'nli.py', 'shortcut.py']\n","```"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":386,"status":"ok","timestamp":1680881899380,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"agEgK0kdrUdT","outputId":"62987e8b-2942-4c27-b0ff-07a1fab78474"},"outputs":[{"name":"stdout","output_type":"stream","text":["['runs', 'nli_data', 'predictions', 'requirements.txt', 'eda.py', 'shortcut.py', 'nli.py', 'testA2.py', 'Assignment2.ipynb', '__pycache__']\n"]}],"source":["import os\n","\n","if USE_COLAB:\n","    # TODO: Fill in the path where you download the Assignment folder into\n","    ROOT_PATH = \"/content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2\"  # Replace with your directory to A2 folder\n","    print(os.listdir(ROOT_PATH))\n"]},{"cell_type":"markdown","metadata":{"id":"_5mABwvHy5-e"},"source":["Before we start, we also need to run some boilerplate code to set up our environment, same as previous assignments. You'll need to rerun this setup code each time you start the notebook."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41808,"status":"ok","timestamp":1680881941187,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"ZhJT7Fo4_D1f","outputId":"1030fbc6-ea1d-4114-d9b4-553804fc5804"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: jsonlines==3.1.0 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 1)) (3.1.0)\n","Requirement already satisfied: transformers==4.26.1 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 2)) (4.26.1)\n","Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 3)) (3.8.1)\n","Requirement already satisfied: apex==0.9.10.dev0 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (0.9.10.dev0)\n","Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 5)) (1.2.2)\n","Requirement already satisfied: huggingface-hub==0.12.1 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 6)) (0.12.1)\n","Requirement already satisfied: numpy==1.22.4 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 7)) (1.22.4)\n","Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 8)) (4.65.0)\n","Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 9)) (1.10.1)\n","Requirement already satisfied: urllib3==1.26.15 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 10)) (1.26.15)\n","Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 11)) (1.16.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from jsonlines==3.1.0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 1)) (22.2.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 2)) (2022.10.31)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 2)) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 2)) (23.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 2)) (2.27.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 2)) (3.10.7)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 2)) (0.13.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 3)) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 3)) (1.1.1)\n","Requirement already satisfied: zope.sqlalchemy in /usr/local/lib/python3.9/dist-packages (from apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (2.0)\n","Requirement already satisfied: wtforms in /usr/local/lib/python3.9/dist-packages (from apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (3.0.1)\n","Requirement already satisfied: cryptacular in /usr/local/lib/python3.9/dist-packages (from apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (1.6.2)\n","Requirement already satisfied: pyramid>1.1.2 in /usr/local/lib/python3.9/dist-packages (from apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (2.0.1)\n","Requirement already satisfied: wtforms-recaptcha in /usr/local/lib/python3.9/dist-packages (from apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (0.3.2)\n","Requirement already satisfied: velruse>=1.0.3 in /usr/local/lib/python3.9/dist-packages (from apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (1.1.1)\n","Requirement already satisfied: pyramid-mailer in /usr/local/lib/python3.9/dist-packages (from apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (0.15.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.2->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 5)) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub==0.12.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 6)) (4.5.0)\n","Requirement already satisfied: webob>=1.8.3 in /usr/local/lib/python3.9/dist-packages (from pyramid>1.1.2->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (1.8.7)\n","Requirement already satisfied: plaster in /usr/local/lib/python3.9/dist-packages (from pyramid>1.1.2->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (1.1.2)\n","Requirement already satisfied: venusian>=1.0 in /usr/local/lib/python3.9/dist-packages (from pyramid>1.1.2->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (3.0.0)\n","Requirement already satisfied: zope.deprecation>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from pyramid>1.1.2->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (5.0)\n","Requirement already satisfied: translationstring>=0.4 in /usr/local/lib/python3.9/dist-packages (from pyramid>1.1.2->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (1.4)\n","Requirement already satisfied: zope.interface>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from pyramid>1.1.2->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (6.0)\n","Requirement already satisfied: plaster-pastedeploy in /usr/local/lib/python3.9/dist-packages (from pyramid>1.1.2->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (1.0.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pyramid>1.1.2->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (67.6.1)\n","Requirement already satisfied: hupper>=1.5 in /usr/local/lib/python3.9/dist-packages (from pyramid>1.1.2->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (1.12)\n","Requirement already satisfied: anykeystore in /usr/local/lib/python3.9/dist-packages (from velruse>=1.0.3->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (0.2)\n","Requirement already satisfied: python3-openid in /usr/local/lib/python3.9/dist-packages (from velruse>=1.0.3->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (3.2.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.9/dist-packages (from velruse>=1.0.3->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: pbkdf2 in /usr/local/lib/python3.9/dist-packages (from cryptacular->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (1.3)\n","Requirement already satisfied: repoze.sendmail>=4.1 in /usr/local/lib/python3.9/dist-packages (from pyramid-mailer->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (4.4.1)\n","Requirement already satisfied: transaction in /usr/local/lib/python3.9/dist-packages (from pyramid-mailer->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (3.1.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 2)) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 2)) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 2)) (3.4)\n","Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.9/dist-packages (from wtforms->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (2.1.2)\n","Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,<2,>=1.1 in /usr/local/lib/python3.9/dist-packages (from zope.sqlalchemy->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (1.4.47)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,<2,>=1.1->zope.sqlalchemy->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (2.0.2)\n","Requirement already satisfied: PasteDeploy>=2.0 in /usr/local/lib/python3.9/dist-packages (from plaster-pastedeploy->pyramid>1.1.2->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (3.0.1)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from python3-openid->velruse>=1.0.3->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (0.7.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib->velruse>=1.0.3->apex==0.9.10.dev0->-r /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/requirements.txt (line 4)) (3.2.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n","Requirement already satisfied: torch==1.13.1+cu116 in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n","Requirement already satisfied: torchvision==0.14.1+cu116 in /usr/local/lib/python3.9/dist-packages (0.14.1+cu116)\n","Requirement already satisfied: torchaudio==0.13.1 in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1+cu116) (4.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1+cu116) (2.27.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1+cu116) (1.22.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1+cu116) (8.4.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1+cu116) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1+cu116) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1+cu116) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1+cu116) (2022.12.7)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.9/dist-packages (0.0.post1)\n"]}],"source":["if USE_COLAB:\n","    requirements = ROOT_PATH + \"/requirements.txt\"\n","    %pip install -r {requirements}\n","    %pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n","    %pip install torchmetrics\n","    %pip install sklearn"]},{"cell_type":"markdown","metadata":{"id":"RUw9ycDa21dl"},"source":["\n","Run this cell to load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1680881941187,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"3PVAoLPQ_I7c","outputId":"77981fdb-32a9-4a5d-e3e4-01789dc13d7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1680881941188,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"mA1Qk-_K_LRm"},"outputs":[],"source":["if USE_COLAB:\n","    from copy import deepcopy\n","    import numpy as np\n","    from tqdm import tqdm\n","    import jsonlines\n","    import sys\n","    import time\n","    import random\n","\n","    import torch\n","    import torch.utils.data\n","    from torch import nn, optim\n","    from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","    from transformers import AdamW, get_constant_schedule_with_warmup\n","    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","\n","    import itertools\n","    import pprint"]},{"cell_type":"markdown","metadata":{"id":"csa48DhDr0td"},"source":["Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n","\n","```\n","Hello A2!\n","```"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1680881941188,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"U10S-b9BrNxj","outputId":"afff637b-ab98-411e-ab76-c38f1581329d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello A2!\n"]}],"source":["if USE_COLAB:\n","    GOOGLE_DRIVE_PATH = ROOT_PATH\n","    sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","    from testA2 import hello_A2\n","\n","    hello_A2()\n"]},{"cell_type":"markdown","metadata":{"id":"dnWpzntscWUE"},"source":["Note that if CUDA is not enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n","\n","The global variables `dtype` and `device` will control the data types throughout this assignment.\n","\n","We will be using `torch.float = torch.float32` for all operations.\n","\n","Please refer to https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype for more details about data types."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680881941188,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"Rqb9cwkNIEHr","outputId":"d6a44d6e-8218-41de-b527-d77dde3292ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Good to go!\n"]}],"source":["if USE_COLAB:\n","    if torch.cuda.is_available():\n","        print(\"Good to go!\")\n","    else:\n","        print(\"Please set GPU via Edit -> Notebook Settings.\")\n"]},{"cell_type":"markdown","metadata":{"id":"9MSpYuMcyHfl"},"source":["### Local Setup\n","If you skip Google Colab setup, you still need to fill in the path where you download the Assignment folder, and install required packages."]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680881941189,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"QQHs7vHhydij"},"outputs":[],"source":["if not USE_COLAB:\n","    ROOT_PATH = \"./\"  # Replace with your directory to A2 folder\n","    %pip install jsonlines==3.1.0\n","    %pip install transformers==4.26.1\n","    %pip install nltk==3.8.1\n","    %pip install apex==0.9.10.dev0\n","    %pip install scikit-learn==1.2.2\n","    %pip install huggingface-hub==0.12.1\n","    %pip install numpy==1.22.4\n","    %pip install tqdm==4.65.0\n","    %pip install scipy==1.10.1\n","    %pip install urllib3==1.26.15\n","    %pip install six==1.16.0\n","    %pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n","    %pip install torchmetrics\n","    %pip install sklearn"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680881941189,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"C6b-Enimyywz","outputId":"e8998158-33ee-411e-e351-2d43e177a870"},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680881941189,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"Q6dgLZ9kyqpO"},"outputs":[],"source":["if not USE_COLAB:\n","    from copy import deepcopy\n","    import numpy as np\n","    from tqdm import tqdm\n","    import jsonlines\n","    import sys\n","    import time, os\n","    import random\n","\n","    import torch\n","    import torch.utils.data\n","    from torch import nn, optim\n","    from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","    from transformers import AdamW, get_constant_schedule_with_warmup\n","    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","    import itertools\n","    import pprint\n"]},{"cell_type":"markdown","metadata":{"id":"rHhgkhaH-IUl"},"source":["<a name=\"1\"></a>\n","## **PART 1: Finetuning DistilBERT for NLI**\n","---"]},{"cell_type":"markdown","metadata":{"id":"tD2YPuqeIYBN"},"source":["### **What is the NLI task?🧐**\n","> Given a pair of sentences, denoted as a \"premise\" sentence and a \"hypothesis\" sentence, NLI (or RTE) aims to determine their logical relationship, i.e. whether they are logically follow (entailment), unfollow (contradiction) or are undetermined (neutral) to each other.\n","\n","> Defined as a machine learning task, NLI can be considered as a 3-classes (entailment, contradiction, or neutral) classification task, with a sentence-pair input (\"hypothesis\" and “premise”).\n","\n","> **You can run the following cell to have the first glance at your data**. Each data sample is a python dictionary, which consists of following components:\n","- premise sentence (*'premise'*), \n","- hypothesis sentence (*'hypothesis'*) \n","- domain (*'domain'*): describing the topic of premise and hypothesis sentences (e.g., government regulations, telephone talks, etc.)\n","- label (*'label'*): indicating the logical relation between premise and hypothesis (i.e., entailment, contradiction, or neutral)."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680881941189,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"p-ODgcNUqYtm","outputId":"eeb86740-90fb-448c-e445-b589dc1d8ad4"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'premise': 'The new rights are nice enough', 'hypothesis': 'Everyone really likes the newest benefits ', 'domain': 'slate', 'label': 'neutral'}\n","{'premise': 'This site includes a list of all award winners and a searchable database of Government Executive articles.', 'hypothesis': 'The Government Executive articles housed on the website are not able to be searched.', 'domain': 'government', 'label': 'contradiction'}\n","{'premise': \"uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him\", 'hypothesis': 'I like him for the most part, but would still enjoy seeing someone beat him.', 'domain': 'telephone', 'label': 'entailment'}\n"]}],"source":["# If you use Google Colab, then data_dir = 'GOOGLE_DRIVE_PATH/nli_data'\n","data_dir = ROOT_PATH + \"/nli_data\"\n","data_dev_path = os.path.join(data_dir, \"dev_in_domain.jsonl\")\n","with jsonlines.open(data_dev_path, \"r\") as reader:\n","    for sid, sample in enumerate(reader.iter()):\n","        print(sample)\n","        if sid == 2:\n","            break\n"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680881941190,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"FF-dRWc7MZlL"},"outputs":[],"source":["# Enter enter your Sciper number\n","SCIPER = \"353068\"\n","seed = int(SCIPER)\n","torch.backends.cudnn.deterministic = True\n"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":503,"status":"ok","timestamp":1680881941685,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"9UPdzLSi4ZVt","outputId":"968fed1e-17b3-4025-fe46-6f036968dca0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Your random seed is:  353068\n"]}],"source":["print(\"Your random seed is: \", seed)\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1410,"status":"ok","timestamp":1680881943094,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"GnX8VC4C0sHW","outputId":"528640b7-f94c-4cd3-f4ae-a3541506da8e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# We use the following pretrained tokenizer and model\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)"]},{"cell_type":"markdown","metadata":{"id":"fCETOFT2dB4u"},"source":["### **1.1 Dataset Processing**\n","Our first step is to load datasets for NLI task by constructing a Pytorch Dataset. Specifically, we will need to implement tokenization and padding with a HuggingFace pre-trained tokenizer.\n","\n","**Complete `NLIDataset` class following the instructions in `nli.py`, and test by running the following cell.**"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7923,"status":"ok","timestamp":1680881951013,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"_5Sya9W5BTDl","outputId":"a18f6d0c-7068-4c8a-928b-ad5c9c42d16b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["9815it [00:07, 1314.88it/s]"]},{"name":"stdout","output_type":"stream","text":["NLIDataset test correct ✅\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from nli import NLIDataset\n","\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","dataset = NLIDataset(ROOT_PATH + \"/nli_data/dev_in_domain.jsonl\", tokenizer)\n","\n","from testA2 import test_NLIDataset\n","\n","test_NLIDataset(dataset)\n"]},{"cell_type":"markdown","metadata":{"id":"W0weQpG6_3vO"},"source":["### **1.2 Model Training and Evaluation**\n","Next, we will implement the training and evaluation process to finetune the model. For model training, you will need to calculate the loss and update the model weights by update the optimizer. Additionally, we add a learning rate schedular to adopt an adaptive learning rate during the whole training process. \n","\n","For evaluation, you will need to compute accuracy and F1 scores to assess the model performance. \n","\n","**Complete the `compute_metric()`, `train()` and `evaluate()` functions following the instructions in the `nli.py` file, you can test compute_metric() by running the following cell.**"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680881951013,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"6w7Leraw4tIY","outputId":"95c5897d-3182-4ca2-dd8b-6c2d0d2afce5"},"outputs":[{"name":"stdout","output_type":"stream","text":["compute_metric test correct ✅\n"]}],"source":["from nli import compute_metrics, train, evaluate\n","\n","from testA2 import test_compute_metrics\n","\n","test_compute_metrics(compute_metrics)\n"]},{"cell_type":"markdown","metadata":{"id":"pvCUS748_3vS"},"source":["#### **Start Training and Validation!**\n","\n","Try the following different hyperparameter settings, compare and discuss the results. (Other hyperparameters should not be changed.)\n","\n","> A. learning_rate 2e-5\n","\n","> B. learning_rate 5e-5\n","\n","**Note:** *Each training will take about 1 hour using a GPU, please keep your computer and notebook active during the training.*\n","\n","**Questions: Which learning rate is better? Explain your answers.**"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131143,"status":"ok","timestamp":1680882082154,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"zn66mMOj_3vS","outputId":"e6548186-f9db-4804-d044-77786a022f02"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["98176it [01:50, 884.68it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["9815it [00:10, 928.51it/s] \n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","model.to(device)\n","\n","train_dataset = NLIDataset(ROOT_PATH + \"/nli_data/train.jsonl\", tokenizer)\n","dev_dataset = NLIDataset(ROOT_PATH + \"/nli_data/dev_in_domain.jsonl\", tokenizer)\n","\n","batch_size = 16\n","epochs = 3\n","max_grad_norm = 1.0\n","warmup_percent = 0.3\n","model_save_root = ROOT_PATH + \"/runs/\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4977566,"status":"ok","timestamp":1680695894241,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"Z5aWqR1h_3vS","outputId":"896e24a3-ad77-4781-a8b2-8435e5ef55dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training with first lr=2e-05\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 6136/6136 [13:26<00:00,  7.61it/s]\n","Evaluation: 100%|██████████| 614/614 [00:24<00:00, 25.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Training Loss: 0.714 | Validation Loss: 0.609\n","Epoch 0 NLI Validation:\n","Accuracy: 75.68% | F1: (79.23%, 70.25%, 76.77%) | Macro-F1: 75.42%\n","Model Saved!\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 6136/6136 [13:23<00:00,  7.64it/s]\n","Evaluation: 100%|██████████| 614/614 [00:24<00:00, 25.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | Training Loss: 0.464 | Validation Loss: 0.594\n","Epoch 1 NLI Validation:\n","Accuracy: 77.63% | F1: (80.78%, 73.40%, 78.25%) | Macro-F1: 77.48%\n","Model Saved!\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 6136/6136 [13:22<00:00,  7.65it/s]\n","Evaluation: 100%|██████████| 614/614 [00:24<00:00, 25.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 | Training Loss: 0.242 | Validation Loss: 0.743\n","Epoch 2 NLI Validation:\n","Accuracy: 76.31% | F1: (79.63%, 72.10%, 77.03%) | Macro-F1: 76.26%\n","Model Saved!\n","\n","\n","================\n","\n","\n","Training with first lr=5e-05\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 6136/6136 [13:29<00:00,  7.58it/s]\n","Evaluation: 100%|██████████| 614/614 [00:24<00:00, 25.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Training Loss: 0.364 | Validation Loss: 0.646\n","Epoch 0 NLI Validation:\n","Accuracy: 76.22% | F1: (79.52%, 71.23%, 77.24%) | Macro-F1: 76.00%\n","Model Saved!\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 6136/6136 [13:21<00:00,  7.65it/s]\n","Evaluation: 100%|██████████| 614/614 [00:24<00:00, 25.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | Training Loss: 0.251 | Validation Loss: 0.846\n","Epoch 1 NLI Validation:\n","Accuracy: 75.98% | F1: (78.60%, 71.82%, 77.45%) | Macro-F1: 75.96%\n","Model Saved!\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 6136/6136 [13:23<00:00,  7.63it/s]\n","Evaluation: 100%|██████████| 614/614 [00:24<00:00, 25.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 | Training Loss: 0.203 | Validation Loss: 1.098\n","Epoch 2 NLI Validation:\n","Accuracy: 75.31% | F1: (78.43%, 70.89%, 76.29%) | Macro-F1: 75.21%\n","Model Saved!\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","lr_option_1 = 2e-5\n","lr_option_2 = 5e-5\n","\n","print(f\"Training with first lr={lr_option_1}\")\n","train(\n","    train_dataset,\n","    dev_dataset,\n","    model,\n","    device,\n","    batch_size,\n","    epochs,\n","    lr_option_1,\n","    warmup_percent,\n","    max_grad_norm,\n","    model_save_root,\n",")\n","\n","print(\"\\n\\n================\\n\\n\")\n","\n","print(f\"Training with first lr={lr_option_2}\")\n","train(\n","    train_dataset,\n","    dev_dataset,\n","    model,\n","    device,\n","    batch_size,\n","    epochs,\n","    lr_option_2,\n","    warmup_percent,\n","    max_grad_norm,\n","    model_save_root,\n",")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ry-R8tTAV1HV"},"source":["**Learning Rate Comparison**\n","\n","The model training with lr=2e-05 has better overall performance (76% accuracy and macro-f1) compared to the model with lr=5e-05 (75% accuracy and macro-f1). \n","This happens because models with lower learning rate tend to converge smoother to a minimum, exploring the loss landscape extensively, and dodging the danger of skipping a good convergence point because of the higher \"jumps\" that a bigger learning rate may cause. However, because of the smaller steps that lower learning rates lead to, the model training might require more time to train."]},{"cell_type":"markdown","metadata":{"id":"wGuzGJCB_3vT"},"source":["### **Fine-Grained Validation**\n","\n","Use the model checkpoint saved under the first hyperparameter setting (learning_rate 2e-5) in 1.4, check the model performance on each domain subsets of the validation set, report the validation loss, accuracy, F1 scores and Macro-F1 on each domain, compare and discuss the results.\n","\n","**Questions: On which domain does the model perform the best? the worst? Give some possible explanations of why the model's best-performed domain is easier, and why the model's worst-performed domain is more challenging. Use some examples to support your explanations.**\n","\n","**Note:** To find examples for supporting your discussion, save the model prediction results on each domain under the './predictions/' folder, by specifying the *result_save_file* of the *evaluate* function."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2946,"status":"ok","timestamp":1680882085098,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"YCWWJjTP_3vT","outputId":"b3b68660-2868-4b4e-d262-e3190d3e676c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Samples in slate: 1955, saved in file: /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/nli_data/dev_in_domain_slate.jsonl\n","Samples in government: 1945, saved in file: /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/nli_data/dev_in_domain_government.jsonl\n","Samples in telephone: 1966, saved in file: /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/nli_data/dev_in_domain_telephone.jsonl\n","Samples in travel: 1976, saved in file: /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/nli_data/dev_in_domain_travel.jsonl\n","Samples in fiction: 1973, saved in file: /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/nli_data/dev_in_domain_fiction.jsonl\n"]}],"source":["batch_size = 16\n","learning_rate = 2e-5\n","warmup_percent = 0.3\n","checkpoint = ROOT_PATH + \"/runs/lr{}-warmup{}\".format(learning_rate, warmup_percent)\n","\n","# Split the validation sets into subsets with different domains\n","# Save the subsets under './nli_data/'\n","# Replace \"...\" with your code\n","\n","raw_data = dev_dataset.text_samples\n","subsets = {}\n","\n","# Gather the subsets in files\n","for index, raw_sample in enumerate(raw_data):\n","    domain = raw_sample[\"domain\"]\n","    if domain in subsets:\n","        subsets[domain] = subsets[domain] + [raw_sample]\n","    else:\n","        subsets[domain] = [raw_sample]\n","\n","# Check prints and dumping data to partial files\n","total = 0\n","for domain in subsets.keys():\n","    filename = ROOT_PATH + \"/nli_data/\" + \"dev_in_domain\" + \"_\" + domain + \".jsonl\"\n","\n","    # Dump to file\n","    with jsonlines.open(filename, \"w\") as writer:\n","        writer.write_all(subsets[domain])\n","\n","    print(f\"Samples in {domain}: {len(subsets[domain])}, saved in file: {filename}\")\n","    total += len(subsets[domain])\n","\n","# Sanity Check\n","assert total == len(raw_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36710,"status":"ok","timestamp":1680707196893,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"Q4J2pu60xHTd","outputId":"1a3d6c10-4690-4571-9abe-63699abeafe9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["1973it [00:00, 4418.74it/s]\n","Evaluation: 100%|██████████| 124/124 [00:19<00:00,  6.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: fiction\n","Validation Loss: 0.734 | Accuracy: 76.53%\n","F1: (78.31%, 72.70%, 78.55%) | Macro-F1: 76.52%\n","Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["1945it [00:00, 2731.55it/s]\n","Evaluation: 100%|██████████| 122/122 [00:23<00:00,  5.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: government\n","Validation Loss: 0.653 | Accuracy: 80.26%\n","F1: (83.78%, 76.22%, 80.48%) | Macro-F1: 80.16%\n","Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["1955it [00:00, 2974.60it/s]\n","Evaluation: 100%|██████████| 123/123 [00:24<00:00,  5.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: slate\n","Validation Loss: 0.873 | Accuracy: 71.20%\n","F1: (74.06%, 67.23%, 72.27%) | Macro-F1: 71.19%\n","Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["1966it [00:00, 3324.58it/s]\n","Evaluation: 100%|██████████| 123/123 [00:27<00:00,  4.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: telephone\n","Validation Loss: 0.765 | Accuracy: 74.57%\n","F1: (78.53%, 68.67%, 76.33%) | Macro-F1: 74.51%\n","Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["1976it [00:00, 2827.14it/s]\n","Evaluation: 100%|██████████| 124/124 [00:22<00:00,  5.49it/s]"]},{"name":"stdout","output_type":"stream","text":["Domain: travel\n","Validation Loss: 0.679 | Accuracy: 78.39%\n","F1: (82.36%, 75.06%, 77.54%) | Macro-F1: 78.32%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n","model = DistilBertForSequenceClassification.from_pretrained(checkpoint)\n","model.to(device)\n","\n","for domain in [\"fiction\", \"government\", \"slate\", \"telephone\", \"travel\"]:\n","    # Evaluate and save prediction results in each domain\n","    # Replace \"...\" with your code\n","    filename = ROOT_PATH + \"/nli_data/\" + \"dev_in_domain\" + \"_\" + domain + \".jsonl\"\n","    dev_domain_dataset = NLIDataset(filename, tokenizer)\n","\n","    predictions_file = ROOT_PATH + \"/predictions/predictions_\" + domain + \".txt\"\n","    dev_loss, acc, f1_ent, f1_neu, f1_con = evaluate(\n","        dev_domain_dataset,\n","        model,\n","        device,\n","        batch_size,\n","        no_labels=False,\n","        result_save_file=predictions_file,\n","    )\n","    macro_f1 = (f1_ent + f1_neu + f1_con) / 3\n","\n","    print(f\"Domain: {domain}\")\n","    print(f\"Validation Loss: {dev_loss:.3f} | Accuracy: {acc*100:.2f}%\")\n","    print(\n","        f\"F1: ({f1_ent*100:.2f}%, {f1_neu*100:.2f}%, {f1_con*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%\"\n","    )\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CMnhl_qEV1HW"},"source":["**Comments**\n","\n","Among the by-domain test sets, the model performs best for the \"government\" domain, with accuracy=80% and macro-f1=80%. Also, the worst performance of the model is for the \"slate\" domain, with accuracy=71% and macro-f1=71%.\n","\n","By analyzing the training data, we see that the samples of \"government\" are approximately 19.000, same as the \"slate\" samples, while the corresponding test sets of both domains are balanced on the entries of samples labelled as {entailment, contradiction, neutral} (approximately 600-700 per class). Moreover, the total testing samples per domain are also balanced, containing approximately 2000 test samples for each domain.\n","\n","Thus, given that both test dataset contain balanced number of classes, and the number of training examples are the same for both domains, an explanation regarding this performance difference could be the following:\n","\n","The training examples that belong to \"government\" domain are representative examples to what the model encounters during training, managing to classify them with better accuracy. On the other hand, this is not the case for \"slate\" domain, as the test samples differ from what the model has seen during training, leading to worst performance when testing. \n","\n","This happens, for example, when the model tries to classify pairs of sentences that are representative examples to what it has encountered during training. \n","\n","This phenomenon shows how important is to have representative and inclusive training data, in order to give the model enough power to classify sentences of different types.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6NyMZ5E4-QxM"},"source":["## **Task2: Identify Shortcuts**\n","\n","We aim to find some shortcuts that the model in 1.4 (under the first hyperparameter setting) has learned."]},{"cell_type":"markdown","metadata":{"id":"4lCHLdaH_3vT"},"source":["### **2.1 Word-Pair Pattern Extraction**\n","\n","We consider to exatrct simple word-pair patterns that the model may have learned from the NLI data. \n","\n","For this, we assume that a pair of words that occur in a premise-hypothesis sentence pair (one occurs in premise and the other occurs in hypothesis) may serve as a key indicator of the logical relationship between the premise and hypothesis sentences. For example:\n","\n",">- Premise: Consider the United States Postal Service.\n",">- Hypothesis: Forget the United States Postal Service.\n","\n","Here the word-pair \"consider\" and \"forget\" determine that the premise and hypothesis have a *contradiction* relationship, so (consider, forget) --> *contradiction* might be a good pattern to learn.\n","\n","**Note:** \n","- We do not consider the naive word pair patterns where the word from premise and the word from hypothesis are identical, e.g., (service, service) got from the above premise-hypothesis sentence pair.\n","- We do not consider stop words neither, punctuations and words that contain special prefix '##', e.g., '##s' in the pattern extraction."]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680882085098,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"7RKdt_-j_3vT","outputId":"5ce8a706-ff53-467d-d3f7-2d39151effc5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# stop_words and puntuations to be removed from consideration in the pattern extraction\n","\n","import nltk\n","\n","nltk.download(\"stopwords\")\n","stop_words = nltk.corpus.stopwords.words(\"english\")\n","stop_words.append(\"uh\")\n","\n","import string\n","\n","puncs = string.punctuation\n"]},{"cell_type":"markdown","metadata":{"id":"_NovYRxv_3vU"},"source":["**Complete `word_pair_extraction()` function in `shortcut.py` file.**\n","\n","The keys of the returned dictionary *word_pairs* should be **different word-pairs** appered in premise-hypothesis sentence pairs, i.e., (a word from the premise, a word from the hypothesis).\n","\n","The value of a word-pair key records the counts of entailment, neutral and contradiction predictions **made by the model** when the word-pair occurs, i.e., \\[#entailment_predictions, #neutral_predictions,  #contradiction_predictions\\].\n","\n","**Note:** Remember to remove naive word pairs (i.e., premise word identical to hypothesis word), stop_words, puntuations and words with special prefix '##' out of consideration."]},{"cell_type":"markdown","metadata":{"id":"UTHt1frZ_3vU"},"source":["### **2.2 Distill Potentially Useful Patterns**\n","\n","Find and print the **top-100** word-pairs that are associated with the **largest total number** of model predictions, which might contain frequently used patterns."]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1680882085098,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"IFZm6tFJFmlg"},"outputs":[],"source":["from shortcut import word_pair_extraction, get_representatives, plot_all"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15990,"status":"ok","timestamp":1680882101086,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"TUyal5mW_3vU","outputId":"df981dc1-bdfd-4551-8197-b1efffbb6be5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top100 result frequent pair IDs:\n","{(2578, 3423): [22, 19, 20], (3423, 2578): [19, 12, 19], (2113, 2051): [23, 13, 10], (10690, 2326): [16, 11, 14], (2326, 10690): [14, 10, 13], (2113, 2111): [17, 17, 2], (2113, 2769): [13, 15, 8], (2113, 2066): [17, 10, 9], (2113, 2843): [17, 13, 5], (2113, 3422): [20, 4, 11], (3398, 2066): [10, 11, 12], (2197, 2086): [15, 6, 10], (2113, 2191): [13, 7, 11], (2113, 2131): [9, 11, 9], (2052, 2071): [6, 11, 11], (2066, 2228): [16, 10, 1], (2113, 2196): [1, 2, 24], (3398, 2028): [7, 7, 13], (2095, 2197): [10, 5, 11], (3398, 2051): [7, 12, 7], (2066, 2843): [11, 11, 3], (2113, 10047): [7, 8, 10], (2113, 2052): [5, 11, 9], (2113, 2228): [5, 17, 3], (2113, 2175): [11, 7, 7], (2028, 2111): [9, 9, 6], (3231, 22423): [8, 8, 8], (3398, 2196): [5, 1, 18], (3398, 2131): [9, 9, 6], (2034, 5653): [4, 10, 9], (2113, 2092): [9, 7, 7], (2113, 2336): [11, 7, 5], (2228, 2052): [4, 8, 10], (2116, 2111): [9, 3, 10], (2052, 2111): [7, 11, 4], (2071, 2052): [6, 4, 12], (2092, 2131): [3, 8, 11], (7494, 10995): [9, 10, 3], (4268, 2336): [11, 5, 6], (2113, 3046): [10, 2, 10], (2028, 2197): [7, 7, 7], (2113, 2202): [6, 7, 8], (2066, 2196): [3, 2, 16], (2113, 4148): [9, 2, 10], (2113, 2126): [13, 5, 3], (2113, 2130): [5, 11, 5], (2028, 2131): [7, 9, 4], (2428, 2052): [5, 11, 4], (2048, 2028): [5, 5, 10], (3046, 2052): [7, 9, 4], (7494, 3167): [8, 10, 2], (2259, 2047): [11, 6, 3], (3423, 2110): [7, 5, 8], (2113, 2154): [6, 6, 8], (2113, 2095): [11, 7, 2], (3398, 2995): [9, 5, 6], (2113, 2428): [6, 3, 11], (2052, 7016): [6, 0, 14], (2028, 2066): [7, 5, 7], (2428, 2066): [7, 6, 6], (5653, 2326): [6, 4, 9], (5653, 10690): [4, 5, 10], (2110, 3423): [7, 5, 7], (5366, 3465): [10, 4, 5], (2197, 2095): [8, 5, 6], (2089, 2453): [10, 8, 1], (2092, 2051): [3, 11, 5], (2130, 2111): [3, 16, 0], (3398, 2428): [8, 6, 5], (2113, 2086): [10, 4, 5], (2113, 4268): [8, 0, 11], (3398, 2228): [7, 6, 6], (2843, 2052): [4, 10, 5], (2183, 2131): [6, 8, 4], (2086, 2095): [6, 4, 8], (2051, 2052): [5, 9, 4], (4921, 2196): [5, 2, 11], (2166, 2878): [6, 12, 0], (2553, 2913): [7, 6, 5], (22423, 3231): [6, 6, 6], (2228, 2903): [11, 4, 3], (3398, 2843): [5, 11, 2], (3398, 10047): [9, 5, 4], (2113, 2071): [11, 4, 3], (3398, 2175): [5, 6, 7], (2028, 2116): [4, 9, 4], (2113, 2146): [9, 5, 3], (2126, 2131): [7, 7, 3], (2204, 2919): [2, 3, 12], (2066, 2071): [6, 7, 4], (10047, 2066): [5, 4, 8], (2120, 7494): [8, 3, 6], (2878, 2166): [6, 11, 0], (15727, 20964): [5, 7, 5], (2047, 2259): [9, 5, 3], (2974, 2592): [8, 4, 5], (2592, 2110): [17, 0, 0], (2175, 2111): [7, 9, 1], (2113, 2518): [8, 8, 1], (3398, 2095): [5, 7, 5]}\n"]}],"source":["def get_topK(dict_data, K=100):\n","    return dict(itertools.islice(dict_data.items(), K))\n","\n","\n","# all your saved model prediction results in 1.2 Fine-Grained Validation\n","prediction_files = [\n","    ROOT_PATH + \"/predictions/predictions_\" + domain + \".txt\"\n","    for domain in [\"fiction\", \"government\", \"slate\", \"telephone\", \"travel\"]\n","]\n","\n","tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n","word_pairs = word_pair_extraction(prediction_files, tokenizer)\n","\n","# find top-100 word-pairs associated with the largest total number of model predictions\n","sorted_pairs = dict(sorted(word_pairs.items(), key=lambda x: sum(x[1]), reverse=True))\n","top_100_freq_pairs = get_topK(sorted_pairs)\n","\n","print(\"Top100 result frequent pair IDs:\")\n","print(top_100_freq_pairs)"]},{"cell_type":"markdown","metadata":{"id":"pz25EvuI_3vU"},"source":["**Among the top-100 frequent word-pairs above**, find out the **top-5** word-pairs whose occurances **most likely** lead to *entailment* predictions (entailment patterns), and the **top-5** word-pairs whose occurances **most likely** lead to *contradiction* predictions (contradiction patterns).\n","\n","**Explain your rules for finding these word pairs.**"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1773,"status":"ok","timestamp":1680882102857,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"pfgYXJpXV1HX","outputId":"6cbc9a0f-31b2-432e-f306-b219dcaf1942"},"outputs":[{"name":"stdout","output_type":"stream","text":["Entailment Patterns:\n","{(2113, 2051): [23, 13, 10], (2578, 3423): [22, 19, 20], (2113, 3422): [20, 4, 11], (3423, 2578): [19, 12, 19], (2113, 2111): [17, 17, 2]}\n","Contradiction Patterns:\n","{(2113, 2196): [1, 2, 24], (2578, 3423): [22, 19, 20], (3423, 2578): [19, 12, 19], (3398, 2196): [5, 1, 18], (2066, 2196): [3, 2, 16]}\n"]}],"source":["# find top-5 entailment and contradiction patterns\n","top_5_entailment_naive = get_topK(\n","    dict(sorted(word_pairs.items(), key=lambda x: x[1][0], reverse=True)), 5\n",")\n","\n","top_5_contradict_naive = get_topK(\n","    dict(sorted(word_pairs.items(), key=lambda x: x[1][2], reverse=True)), 5\n",")\n","\n","print(\"Entailment Patterns:\")\n","print(top_5_entailment_naive)\n","print(\"Contradiction Patterns:\")\n","print(top_5_contradict_naive)"]},{"cell_type":"markdown","metadata":{"id":"WgHgiqh9V1HY"},"source":["The task above is a naive implementation of the question, that just sorts and gets the top 5 entries that have the highest number of occurences in samples of contradiction and entailment.\n","\n","We see that although this method returns the shortcut pairs that have many occurences in the desired classification type, many entries have many occurences in all possible classes, which does not guarantee that the model will classify them as the class that has the highest number of counts. \n","\n","Thus, we need to develop a better way to locate such entries."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2033,"status":"ok","timestamp":1680882104889,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"Yq2cVOaWTEYw","outputId":"d549c2bb-5c54-49e4-f4ca-109dd7f04707"},"outputs":[{"name":"stdout","output_type":"stream","text":["Entailment Patterns:\n","{(2592, 2110): [17, 0, 0], (7340, 5324): [11, 0, 0], (2113, 2307): [12, 0, 2], (2592, 5836): [10, 0, 0], (2592, 2708): [10, 0, 0]}\n","Contradiction Patterns:\n","{(2113, 2196): [1, 2, 24], (3398, 2196): [5, 1, 18], (2066, 2196): [3, 2, 16], (2051, 2196): [0, 2, 12], (2092, 2196): [2, 1, 13]}\n"]}],"source":["# find top-5 entailment and contradiction patterns\n","top_5_entailment = get_topK(\n","    dict(\n","        sorted(\n","            word_pairs.items(),\n","            key=lambda x: (x[1][0] - (x[1][1] + x[1][2])),\n","            reverse=True,\n","        )\n","    ),\n","    5,\n",")\n","\n","top_5_contradict = get_topK(\n","    dict(\n","        sorted(\n","            word_pairs.items(),\n","            key=lambda x: (x[1][2] - (x[1][1] + x[1][0])),\n","            reverse=True,\n","        )\n","    ),\n","    5,\n",")\n","\n","print(\"Entailment Patterns:\")\n","print(top_5_entailment)\n","print(\"Contradiction Patterns:\")\n","print(top_5_contradict)"]},{"cell_type":"markdown","metadata":{"id":"H8RiUyQKV1HZ"},"source":["The snippet above is a rule-based implementation of the question. It tries to find the shortcut pairs that seem to appear mostly in only one class case. This is implemented by substracting from the counter we are interested the other two counters.\n","\n","We see that we actually get shortcut pairs that appear only or mostly in only one type of class, which means that this method is sufficient for our task."]},{"cell_type":"markdown","metadata":{"id":"wjVti-vL_3vV"},"source":["### **2.3 Case Study**\n","\n","Find out and study **4 representative** cases where the pattern that you have found in 2.2 **fails**, e.g., the premise-hypothesis sentence pair contains ('good', 'bad'), but has an *entailment* gold label.\n","\n","**Based on your case study, explain the limitations of the word-pair patterns.**"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1680882104889,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"2XyPmb01_3vV","outputId":"cdde24e8-fbc1-47df-d7ce-c1f85db28415"},"outputs":[{"name":"stdout","output_type":"stream","text":["Decoded Entailment Pairs:\n","[('information', 'state', 'entailment', [17, 0, 0]),\n"," ('shops', 'stores', 'entailment', [11, 0, 0]),\n"," ('know', 'great', 'entailment', [12, 0, 2]),\n"," ('information', 'represents', 'entailment', [10, 0, 0]),\n"," ('information', 'chief', 'entailment', [10, 0, 0])]\n","\n"," ======= \n","\n","Decoded Contradiction Pairs:\n","[('know', 'never', 'contradiction', [1, 2, 24]),\n"," ('yeah', 'never', 'contradiction', [5, 1, 18]),\n"," ('like', 'never', 'contradiction', [3, 2, 16]),\n"," ('time', 'never', 'contradiction', [0, 2, 12]),\n"," ('well', 'never', 'contradiction', [2, 1, 13])]\n"]}],"source":["# you can fill your code for finding cases here\n","\n","# First, we decode the results to be able to understand the results better and interact\n","# with the dataset easily.\n","\n","# The analysis below is based on the top5 entries retrieved in previous task.\n","id_to_label = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","decoded_entailment = {}\n","for entry in top_5_entailment:\n","    tokens = tokenizer.convert_ids_to_tokens([entry[0], entry[1]])\n","    tok1 = tokens[0]\n","    tok2 = tokens[1]\n","    decoded_entailment[(tok1, tok2)] = [\"entailment\", top_5_entailment[entry]]\n","decoded_entailment = [\n","    (d[0][0], d[0][1], d[1][0], d[1][1]) for d in decoded_entailment.items()\n","]\n","\n","decoded_contradiction = {}\n","for entry in top_5_contradict:\n","    tokens = tokenizer.convert_ids_to_tokens([entry[0], entry[1]])\n","    tok1 = tokens[0]\n","    tok2 = tokens[1]\n","    decoded_contradiction[(tok1, tok2)] = [\"contradiction\", top_5_contradict[entry]]\n","decoded_contradiction = [\n","    (d[0][0], d[0][1], d[1][0], d[1][1]) for d in decoded_contradiction.items()\n","]\n","\n","print(\"Decoded Entailment Pairs:\")\n","pprint.pprint(decoded_entailment)\n","\n","print(\"\\n ======= \\n\")\n","print(\"Decoded Contradiction Pairs:\")\n","pprint.pprint(decoded_contradiction)"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"executionInfo":{"elapsed":630,"status":"ok","timestamp":1680882105519,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"0qvPRXAWV1Ha","outputId":"2448080a-c69b-4dc2-bceb-f5f339ca0ec2"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Now, we can plot them to see more clearly what is going on with our\n","# selected pairs\n","\n","plot_all(decoded_entailment, decoded_contradiction)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VIQamzTGV1Ha"},"source":["Now, after the initial analysis we did, we traverse the dataset to find the 4 representative cases of the shortcut phenomenon. Indeed, we seek cases where the model encountered one of the top5 (entailment or contradiction) pairs we found above, but made a wrong prediction because the label was actually different.\n","\n","Please note that I found 4 examples of sample that have different gold label than what the shortcut indicates, and the model also fails to classify the sample correctly, whenever it was possible."]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8209,"status":"ok","timestamp":1680882115386,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"M1JVlk20V1Ha","outputId":"4dfbdd09-c8b2-4368-e680-e9a8097806c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["-Entailment case for pair ('know', 'great', 'entailment', [12, 0, 2]):\n","{'domain': 'telephone',\n"," 'hypothesis': \"We have money on there, which isn't great\",\n"," 'label': 'contradiction',\n"," 'prediction': 'entailment',\n"," 'premise': 'you know we keep a couple hundred dollars um if that much charged '\n","            \"on those which isn't too bad it's just your normal\"}\n","\n","{'domain': 'telephone',\n"," 'hypothesis': 'Life will be great for subsequent generations if our '\n","               'generation goes without debt.',\n"," 'label': 'neutral',\n"," 'prediction': 'entailment',\n"," 'premise': \"yes they would they just wouldn't be able to own the kind of \"\n","            'automobiles that they think they deserve to own or the kind of '\n","            'homes that we think we deserve to own we might have to you know '\n","            'just be able to i think if we a generation went without debt then '\n","            'the next generation like if if our our generation my husband and '\n","            \"i we're twenty eight if we lived our lives and didn't become you \"\n","            'know indebted like you know our generation before us that um the '\n","            'budget would balance and that we became accustomed to living with '\n","            \"what we could afford which we wouldn't be destitute i mean we \"\n","            \"wouldn't be living on the street by any means but just compared \"\n","            'to how spoiled we are we would be in our own minds but i feel '\n","            'like the generation after us would oh man it it would be so good '\n","            \"it would be so much better it wouldn't be perfect but then they \"\n","            'could learn to live with what what they could afford to save to '\n","            'buy and if you want a nicer car than that well you save a little '\n","            'longer'}\n"]}],"source":["# Entailement cases\n","print(f\"-Entailment case for pair {decoded_entailment[2]}:\")\n","reps = get_representatives(prediction_files, decoded_entailment[2], tokenizer, True, 2)\n","\n","pprint.pprint(reps[0])\n","print()\n","pprint.pprint(reps[1])\n"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20415,"status":"ok","timestamp":1680882135798,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"MZDZM1gmV1Ha","outputId":"32e5ae24-c8e4-4ca3-e28a-a06298bee51b"},"outputs":[{"name":"stdout","output_type":"stream","text":["-Contradiction case for pair ('yeah', 'never', 'contradiction', [5, 1, 18]):\n","{'domain': 'telephone',\n"," 'hypothesis': \"That's amazing, I've never seen anything like it.\",\n"," 'label': 'neutral',\n"," 'prediction': 'contradiction',\n"," 'premise': \"uh-huh uh-huh uh-huh yeah well that's really neat\"}\n","\n","-Contradiction case for pair ('well', 'never', 'contradiction', [2, 1, 13]):\n","{'domain': 'telephone',\n"," 'hypothesis': \"That's amazing, I've never seen anything like it.\",\n"," 'label': 'neutral',\n"," 'prediction': 'contradiction',\n"," 'premise': \"uh-huh uh-huh uh-huh yeah well that's really neat\"}\n","\n","{'domain': 'telephone',\n"," 'hypothesis': 'Camping is something that I never got the hang of, though I '\n","               'took the children fishing, we never did do camping.',\n"," 'label': 'entailment',\n"," 'prediction': 'entailment',\n"," 'premise': 'well camping is one thing that i i could never get used to uh i i '\n","            'used to take the kids to go fishing and things like that but i '\n","            'never went uh never went camping'}\n"]}],"source":["# Contradiction cases\n","print(f\"-Contradiction case for pair {decoded_contradiction[1]}:\")\n","pprint.pprint(\n","    get_representatives(prediction_files, decoded_contradiction[1], tokenizer, True, 1)[\n","        0\n","    ]\n",")\n","\n","print(f\"\\n-Contradiction case for pair {decoded_contradiction[4]}:\")\n","pprint.pprint(\n","    get_representatives(prediction_files, decoded_contradiction[4], tokenizer, True, 1)[\n","        0\n","    ]\n",")\n","print()\n","pprint.pprint(\n","    get_representatives(\n","        prediction_files, decoded_contradiction[4], tokenizer, False, 3\n","    )[1]\n",")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TM01xxudV1Ha"},"source":["**Interpretation**\n","- ('know', 'great'): Entailment. We provide two samples that indicate the shortcut failure (in the corresponding output cell above). \n","    * The first sample, although it contains the entailment shortcut pair, it is labelled as a contradiction, and the model also fails to classify it correctly. \n","    * The second sample, is a similar case as the first, because it is labelled as neutral, but the model classifies it as an entailment case, which is wrong.\n","\n","- ('yeah', 'never'): Contradiction. We provice a sample that indicates the fail of the contradiction shortcut (in the correspodning cell above). We see that the sentence is labelled as neutral, but the model fails and classifies it as a contradiction, as the shortcut indicates.\n","\n","- ('well', 'never'): Contradiction. Same as above, we provide a sample that is again labelled as a neutral case, but the model classifies it as a contradiction. Interestingly, this sample is the same sample of ('yeah', 'never') pair, so we located a double failure. Note that in order to be complete, we provide another available sample we found. The second sample is a case were the relationship of the sentences is classified as an entailment, but the model manages to classify it correctly.\n","\n","**Limitations of Shortcuts**\n","\n","As we see, there can be cases that shortcuts fail to encapsulate the relationship of all the sentences that they appear. This limitation of such word-pairs is expected, as it is very difficult to include the whole relation of two sentences with just a pair of words. Such pairs are not able to provide the whole context between two sentences, which is an important drawback/limitation, because when we compare two sentences to determine their relationship, the overall context of the text is what gives the most reliable answer in the classification between neutral, contradiction and entailment classes."]},{"cell_type":"markdown","metadata":{"id":"yND0DEfT-eXn"},"source":["## **Task3: Annotate New Data**\n","\n","To check the robustness of developed model, **some additional sets of test data** are collected (under /nli_data/test_data/), which contain NLI samples that are out of the domains of the training and validation data.\n","\n","However, the test data does not have gold labels of the relationships between premise and hypothesis sentences, i.e., all the labels are marked as *hidden*. **We consider to annotate the data by ourselves.**"]},{"cell_type":"markdown","metadata":{"id":"ZQNXrRHr_3vV"},"source":["### **3.1 Write an Annotation Guideline**\n","\n","Imagine that you are going to assign this annotation task to a crowdsourcing worker, who is completely not familiar with computer science and NLP. Think about how you are going to explain this annotation task to him in order to guide him do a decent job. Write an annotation guideline for such a worker who are going to do this task for you.\n","\n","**Note:** You should come up with your own guideline without the help of your partner(s) in later Task 3.2"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pfkqNbUA_3vV"},"source":["**Data Annotation Description**\n","\n","In order to assign a data annotation task to the crowd, which may be unfamiliar with NLP and computer science, one needs to design a concrete set of rules and guidelines in order to define how the sentences should be labelled.\n","\n","First of all, we shall give a clear description about what the task is and what we expect from the crowd to do:\n","\n","- Task Description: We have gathered multiple pairs of sentences from different sources, and we want to label their relationship, by deciding if two sentences are a case of contradiction, entailment or neutral relationship. You will be given a set of such pairs, and you should fill the missing \"label\" field with the relation that best describes these two sentences.\n","\n","Next, we should explain what entailment, contradiction and neutral relation is, so that it is clear and has the same meaning for all participants.\n","\n","- Sentence relationship types: We mentioned above that the three possible relationships that the sentence share are: Entailment, Contradiction and Neutral. Below, you will find a description about each type/class:\n","    * Entailment: Sentences that one implies the other. For example, the sentences \"I like apples\" and \"I love apples\" would be annotated as entailment. This category also includes sentences that one paraphrases the other.\n","    * Contradiction: Sentences that have opposite meaning. For example, the sentences \"I am feeling good\" and \"I am feeling bad\" should be annotated as contradiction.\n","    * Neutral: Sentences that share no logical relationship. For example, the sentences \"The sky is blue\" and \"I like books\" would be annotated as neutral.\n","\n","Now that we have described the task and the three possible classes of sentences, we are ready to give our annotation guideline.\n","\n","- Annotation Guideline:\n","    1. Read multiple types the given pair of sentences, and try to fully understand their meaning, based on the whole context of each sentence.\n","    2. Following the aforementioned rules, try to determine if those sentences have opposite meanings, if they imply each other or if there is no logical relationship between them.\n","        * If you are not sure, you may suppose that they share \"neutral\" relationship.\n","        * Please be extremely careful when extracting the meaning of a sentence: You may focus on the whole sentence, not in single words.\n","    3. Based on your class selection (\"entailment\", \"contradition\", \"neutral\"), fill the field \"gold_label\" of the corresponding pair.\n","    4. Repeat the guideline steps for all the pairs assigned to you."]},{"cell_type":"markdown","metadata":{"id":"XBWK4Bw__3vV"},"source":["### **3.2 Annotate Your 100 Datapoints with Partner(s)**\n","\n","Annotate your 100 test datapoints with your partner(s), by editing the value of the key \"label_student1\", \"label_student2\" and \"label_student3\" (if you are in a group of three students) in each datapoint."]},{"cell_type":"markdown","metadata":{"id":"cczFQm0eHknv"},"source":["**Note:** \n","- You can download the assigned annotation file (`<your-testset-id>.jsonl`) by [this link](https://drive.google.com/drive/folders/146ExExmpnSUayu6ArGiN5gQzCPJp0myB?usp=share_link)\n","- Please find your annotation partner according to the \"Student Pairing List for A2 Task3\" shared on Ed."]},{"cell_type":"markdown","metadata":{"id":"IWhjTn2fQ5YE"},"source":["**Name your annotated file as `<index>-<sciper_number>.jsonl`.** \n","\n","For example, if you get `01.jsonl` to annotate, you should name your deliverable as `01-<your_sciper_number>.jsonl`."]},{"cell_type":"markdown","metadata":{"id":"uyP0GtHe_3vW"},"source":["### **3.3 Agreement Measure**\n","\n","Based on your and your partner's annotations on the 100 test datapoints in 3.2, calculate the [Cohen's Kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa) or [Krippendorff's Alpha](https://github.com/pln-fing-udelar/fast-krippendorff) (if you are in a group of three students) between the annotators. Discuss the agreement measure results.\n","\n","**Note:** Cohen's Kappa or Krippendorff's Alpha interpretation\n","\n","0: No Agreement\n","\n","0 ~ 0.2: Slight Agreement\n","\n","0.2 ~ 0.4: Fair Agreement\n","\n","0.4 ~ 0.6: Moderate Agreement\n","\n","0.6 ~ 0.8: Substantial Agreement\n","\n","0.8 ~ 1.0: Near Perfect Agreement\n","\n","1.0: Perfect Agreement\n","\n","> **Questions**: What is your interpretation of Cohen's Kappa or Krippendorff's Alpha value according to the above mapping? Which kind of disagreements are most frequently happen between you and your partner(s), i.e., *entailment* vs. *neutral*, *entailment* vs. *contradiction*, or *neutral* vs. *contradiction*? For the second question, give some examples to explain why that is the case. Are there possible ways to address the disagrrements between two annotators?"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":553,"status":"ok","timestamp":1680882136348,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"CpS6VOiX_3vW","outputId":"041760b5-8ad5-4dff-cb32-82bb5df7d3cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cohen Kappa Score: 0.5219737856592135\n","\n","=========\n","\n","Disagreement Classes (%)\n","    neutral vs contradiction = 0.16129032258064516%\n","    neutral vs entailment = 0.5806451612903226%\n","    contradiction vs entailment = 0.25806451612903225%\n","Total misses: 31\n","\n","========= \n","\n","Indicative examples for neutral-entailment disagreements\n","     prem: And, I kept on, and finally when I saw that life was really expensive, and I didn't have, I couldn't work anymore since I'm not a professional, then I decided to come (to the US), because also, well, the, there in that area, since now there wasn't anywhere to work, I had left hidden from the guerrillas, and, well, that's what I continued.\n","     hyp: I was nervous about coming to the US.\n","\n","=========\n","\n","Indicative examples for contradiction-entailment disagreements\n","     prem: Perhaps we ought to be saying street person for the first and working stiff for the second, but the first has been pre-empted and it seems a little incongruous to find anyone actively seeking to be called a  stiff. \n","     hyp: Street person out to be the characterization for the second. \n","\n","     prem: A number of repairs and alterations will have to be completed at the present facility to assure their usability for a few more years.\n","     hyp: The facility is doing fine, no help is needed.\n","\n","     prem: Kind of letting them down.\n","     hyp: Really impressed them.\n","\n"]}],"source":["# fill your code here\n","\n","label_to_id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","id_to_label = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n","\n","annotated_file_1 = ROOT_PATH + \"/nli_data/test_data/05-nathan.jsonl\"\n","annotated_file_2 = ROOT_PATH + \"/nli_data/test_data/05-353068.jsonl\"\n","data = []\n","\n","annotations1 = []\n","with jsonlines.open(annotated_file_1, \"r\") as reader:\n","    for pred in reader.iter():\n","        annotations1.append(label_to_id[pred[\"label_student1\"]])\n","        data.append(pred)\n","\n","annotations2 = []\n","with jsonlines.open(annotated_file_2, \"r\") as reader:\n","    for pred in reader.iter():\n","        annotations2.append(label_to_id[pred[\"label_student2\"]])\n","\n","# Calculate cohen score\n","from sklearn.metrics import cohen_kappa_score\n","\n","cohscore = cohen_kappa_score(annotations1, annotations2)\n","print(f\"Cohen Kappa Score: {cohscore}\")\n","\n","print(\"\\n=========\\n\")\n","\n","# Calculate all misses\n","# Disagreement-types: (neutral-contradiction)=(1,2), (neutral-entailment)=(1,0), (contradiction-entrailment)=(2,0)\n","misses = {(1, 2): 0, (1, 0): 0, (2, 0): 0}\n","total_misses = 0\n","examples = {(1, 2): [], (1, 0): [], (2, 0): []}\n","for index, entry in enumerate(zip(annotations1, annotations2)):\n","    a1 = entry[0]\n","    a2 = entry[1]\n","\n","    if a1 != a2:\n","        if (a1, a2) in misses:\n","            misses[(a1, a2)] = misses[(a1, a2)] + 1\n","            examples[(a1, a2)] = examples[(a1, a2)] + [(data[index])]\n","        else:\n","            misses[(a2, a1)] = misses[(a2, a1)] + 1\n","            examples[(a2, a1)] = examples[(a2, a1)] + [(data[index])]\n","\n","        total_misses += 1\n","\n","for entry in misses:\n","    misses[entry] = misses[entry] / total_misses\n","\n","print(\"Disagreement Classes (%)\")\n","for entry in misses:\n","    c1 = id_to_label[entry[0]]\n","    c2 = id_to_label[entry[1]]\n","    print(f\"    {c1} vs {c2} = {misses[entry]}%\")\n","print(f\"Total misses: {total_misses}\")\n","print(\"\\n========= \\n\")\n","\n","# Get some examples of neutral VS entailment disagreements\n","\n","all_examples = examples[(1, 0)]  # (1,0) is the neutral vs entailment IDs\n","ex_s = random.sample(all_examples, 1)\n","print(\"Indicative examples for neutral-entailment disagreements\")\n","for ex in ex_s:\n","    print(\"     prem:\", ex[\"premise\"])\n","    print(\"     hyp:\", ex[\"hypothesis\"])\n","    print()\n","\n","print(\"=========\\n\")\n","\n","all_examples = examples[(2, 0)]\n","ex_s = random.sample(all_examples, 3)\n","print(\"Indicative examples for contradiction-entailment disagreements\")\n","for ex in ex_s:\n","    print(\"     prem:\", ex[\"premise\"])\n","    print(\"     hyp:\", ex[\"hypothesis\"])\n","    print()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Iw84ItJkV1Hb"},"source":["**Interpretation of Cohen Kappa Score**\n","\n","The score we got from the comparison of the annotations is 0.52, which corresponds to moderate agreement. This means that there is a moderate level of agreement between the two annotators, compared to the scenario that two annotators would agree on decisions by chance. \n","\n","**Misses Analysis**\n","\n","Out of 100 annotations, we disagreed in 31 in total. Out of this 31, 16% were neutral-contradiction, 58% were neutral-entailment and 25% were entailment-contradiction. \n","\n","The reasons of disagreement can be different and multiple. Indeed, some of them had to do with common mistakes when determining the relationship of obvious examples. But, apart from that, the reasons of disagreement were also based on the type of relationships and cases where sentence relationships were open for interpretation.\n","\n","<u>About the neutral-contradiction and (especially) neutral-entailment</u>. They had to do with scenarios that were open for interpretation and the contradiction-entailment relationship between the sentences was slightly present, leaving space to determine them as neutral. For example:\n","\n","* <u>premise</u>: And, I kept on, and finally when I saw that life was really expensive, and I didn't have, I couldn't work anymore since I'm not a professional, then I decided to come (to the US), because also, well, the, there in that area, since now there wasn't anywhere to work, I had left hidden from the guerrillas, and, well, that's what I continued. \n","* <u>hypothesis</u>: I was nervous about coming to the US.\n","\n","This an example that one annotator labelled this as a neutral relationship, while the other labelled it as an entailment. \n","In fact, both choices make sense for this pair. Labelling as a neutral relationship is reasonable, because there is no straight indication that the man is actually nervous about coming to the US in the text, but based on the syntactic method and the presence of the words \"well\" etc. we could identify this pair as an entailment.\n","\n","\n","<u>About the contradiction-entailment disagreements</u>, there were cases that double negations or the use of vocabulary could lead us to different interpretations of the relationship between two sentences. For example:\n","\n","* <u>premise</u>:Um, if I was able to get educational opportunities in school, I would transfer, but I'm happy with school here, with my sorority, with the city, with, I mean I'm happy with the way things are right now, so probably when I graduate.\n","* <u>hypothesis</u>: I'd prefer not to transfer even if I could I'm quite enjoying myself with school and stuff here.\n","\n","This is an example of a contradiction-entailment disagreement between the annotators. \n","Again, labelling this pair as an entailment makes sense, because in both sentences it is stated that the speaker loves the school and the life in the current place. On the other hand, it stated in the premise that the speaker would transfer, whereas in the hypothesis it is stated that the speaker would prefer not to transfer. Based on that, it would also make sense to label this as a contradiction.\n","\n","Both examples above show that there are cases that could have multiple intepretations leading to different annotations.\n","\n","\n","**Resolving the differences**\n","\n","In order to create the test file with correct annotation labels, we followed the pipeline described below.\n","1. We revised our annotations to correct obvious cases of misses were the relationship between the sentences was obvious.\n","2. For the samples we still disagreed, we asked a third person to help us by choosing what label (between the two) seemed to be more appropriate, in order to have a majority, and then we used this label as the final correct annotation.\n","\n","-The final annotations are available in /nli_data/test_data/05-final.jsonl\n","-My annotations are available in the file /nli_data/test_data/05-353068.jsonl\n","\n","\n","Note: Because my partner has already annotated dataset 05, we proceeded with that instead of 03.\n"]},{"cell_type":"markdown","metadata":{"id":"Ff12-RLf_3vW"},"source":["### **3.4 Robustness Check**\n","\n","Take into account both your and your partner's annotations, determine the final labels of the 100 test datapoints, by editing the value of the key \"label\" in each of your datapoint.\n","\n","Evaluate the performance of your developed model in 1.4 (still under the first hyperparameter setting) on your annotated 100 test datapoints, and compare with the model performance on the validation set.\n","\n","> **Question**: Do you think that your developed model has a good robuestness of handling out-of-domain NLI predictions?"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5301,"status":"ok","timestamp":1680882141647,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"LowJ05h6ypaA","outputId":"321e362c-eb80-4f86-b681-d425ccfba40b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["100it [00:00, 198.37it/s]\n","Evaluation: 100%|██████████| 7/7 [00:01<00:00,  6.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Annotated data results\n","Accuracy: 0.68\n","Macro-f1: 0.6757310032844543\n"]}],"source":["# fill your code here\n","batch_size = 16\n","learning_rate = 2e-5\n","warmup_percent = 0.3\n","checkpoint = ROOT_PATH + \"/runs/lr{}-warmup{}\".format(learning_rate, warmup_percent)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n","model = DistilBertForSequenceClassification.from_pretrained(checkpoint)\n","model.to(device)\n","\n","filename = ROOT_PATH + \"/nli_data/test_data/05-final.jsonl\"\n","dev_domain_dataset = NLIDataset(filename, tokenizer)\n","dev_loss, acc, f1_ent, f1_neu, f1_con = evaluate(\n","    dev_domain_dataset,\n","    model,\n","    device,\n","    batch_size,\n","    no_labels=False,\n","    result_save_file=ROOT_PATH + \"/predictions/predictions_final_annotations.jsonl\",\n",")\n","macro_f1 = (f1_ent + f1_neu + f1_con) / 3\n","\n","print(\"Annotated data results\")\n","print(f\"Accuracy: {acc}\")\n","print(f\"Macro-f1: {macro_f1}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b2fsGijVV1Hc"},"source":["**Performance on new data**\n","\n","\n","We produced the final annotated file with the new test data following the pipeline described in the previous question and we evaluated the model again.\n","\n","Both accuracy and macro-f1 are worse in the case of unknown data, compared to our test datasets, as they contain cases not seen during training, caused by unseen cases and data.\n","\n","We could improve the performance of the model if we use bigger training sets (or augment our current set), to train the model with more possible cases."]},{"cell_type":"markdown","metadata":{"id":"t4wuRpHt-rQF"},"source":["## **Task4: Data Augmentation**\n","\n","Finally, we consider to use a data augmentation method to create more training data, and use the augmented data to improve the model performance. The data augmentation method we are going to use is [EDA](https://aclanthology.org/D19-1670/)."]},{"cell_type":"markdown","metadata":{"id":"FEtgwKJt0kfO"},"source":["### **4.1 EDA: Easy Data Augmentation algorithm for Text**\n","\n","For this section, we will need to implement the most simple data augmentation techniques on textual sentences, including **SR** (Synonym Replacement), **RD** (Random Deletion), **RS** (Random Swap), **RI** (Random Insertion). \n","\n","You should complete all the functions in `eda.py` script, and you can test them with a simple testcase by running the following cell."]},{"cell_type":"markdown","metadata":{"id":"djFbjk31AR0M"},"source":["- **Synonym Replacement (SR)**\n","> In Synonym Replacement, we randomly replace some words in the sentence with their synonyms."]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":499,"status":"ok","timestamp":1680882142145,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"G2ZExbEb03IN","outputId":"56ba22b8-9c25-4315-97a8-9269221948c1"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["import nltk\n","\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","\n","from nltk.corpus import wordnet\n"]},{"cell_type":"markdown","metadata":{"id":"ZyJi-zYyIqsq"},"source":["You can test whether you get the synonyms right and see an example with synonym replacement."]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2798,"status":"ok","timestamp":1680882144942,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"ZMRmcZtx81R4","outputId":"e4fa14df-3077-4a22-e24b-9cdc212cb42e"},"outputs":[{"name":"stdout","output_type":"stream","text":["The synonyms for the word \"task\" are:  ['chore', 'undertaking', 'tax', 'job', 'project', 'labor']\n"]}],"source":["from eda import get_synonyms\n","from testA2 import test_get_synonyms\n","\n","test_get_synonyms(get_synonyms)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680882144942,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"AjFGy4DLFziY","outputId":"1abd7d02-fac8-436d-e714-6d441f7f88cd"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Example of Synonym Replacement: hey military personnel how are you doing\n"," Example of Synonym Replacement: hey humans how are you doing\n"]}],"source":["from eda import synonym_replacement\n","\n","print(\n","    f\" Example of Synonym Replacement: {synonym_replacement('hey man how are you doing',3)}\"\n",")\n","\n","print(\n","    f\" Example of Synonym Replacement: {synonym_replacement('hey man how are you doing',1)}\"\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"IfdHwyxaJXUn"},"source":["- **Random Deletion (RD)**\n","\n","> In Random Deletion, we randomly delete a word if a uniformly generated number between 0 and 1 is smaller than a pre-defined threshold. This allows for a random deletion of some words of the sentence."]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680882144944,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"76bVm640Msa7","outputId":"652324da-63df-49c0-b597-f3f03f35f67e"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Example of Random Deletion: hey man how you doing\n"]}],"source":["from eda import random_deletion\n","\n","print(\n","    f\" Example of Random Deletion: {random_deletion('hey man how are you doing', p=0.3, max_deletion_n=3)}\"\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"Ajqx9cABNk5a"},"source":["- **Random Swap (RS)**\n","> In Random Swap, we randomly swap the order of two words in a sentence."]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680882144944,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"6Vuott-vQn6W","outputId":"f347bf1c-b20b-4d90-a2e6-4a64f962ba67"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Example of Random Swap: hey man you are how doing\n"]}],"source":["from eda import swap_word\n","\n","print(f\" Example of Random Swap: {swap_word('hey man how are you doing')}\")"]},{"cell_type":"markdown","metadata":{"id":"WSpcGqdfQ78_"},"source":["- **Random Insertion (RI)**\n","> Finally, in Random Insertion, we randomly insert synonyms of a word at a random position.\n","> Data augmentation operations should not change the true label of a sentence, as that would introduce unnecessary noise into the data. Inserting a synonym of a word in a sentence, opposed to a random word, is more likely to be relevant to the context and retain the original label of the sentence."]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1680882145850,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"G5itS2lJRmvV","outputId":"9e0cadf8-a46c-448c-ec43-1927b08c767b"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Example of Random Insertion: hey man how are humans mankind you doing\n"]}],"source":["from eda import random_insertion\n","\n","print(\n","    f\" Example of Random Insertion: {random_insertion('hey man how are you doing', n=2)}\"\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"KGOlMl3n1SyJ"},"source":["### **4.2 Augment Your Model**\n","\n","Combine all the functions you have implemented in 4.1, you can come up with your own data augmentation pipeline with various p and n ;)\n","\n","Next step is to expand the training data you used in Task1, re-train your model in 1.4 on your augmented data, and re-evaluate its performance on both the given validation set as well as on your manually annotated 100 test datapoints. \n","\n","Discuss the improvements that your data augmentation brings to your model. ***Include some examples of old vs. new model predictions to demonstrate the improvements.***\n","\n","**Warning: In terms of data size and training time control, we stipulate that your augmented training data should not be larger than 100M.** (Currently the training data train.jsonl is about 25M.)"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680882145850,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"d0pTcUsYzsGZ","outputId":"ccb7596f-6c80-46a5-fcd4-433bcdddfbf8"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Original Sentence : hey man how are you doing\n"," SR Augmented Sentence : hey valet de chambre how are you doing\n"," RD Augmented Sentence : hey man how are doing\n"," RS Augmented Sentence : hey man how are you doing\n"," RI Augmented Sentence : humanity hey man how are man you doing\n"]}],"source":["def aug(sent, n, p):\n","    print(f\" Original Sentence : {sent}\")\n","    print(f\" SR Augmented Sentence : {synonym_replacement(sent, n)}\")\n","    print(f\" RD Augmented Sentence : {random_deletion(sent, p, n)}\")\n","    print(f\" RS Augmented Sentence : {swap_word(sent)}\")\n","    print(f\" RI Augmented Sentence : {random_insertion(sent,n)}\")\n","\n","\n","aug(\"hey man how are you doing\", p=0.2, n=2)\n"]},{"cell_type":"markdown","metadata":{"id":"l-0U-CD323iY"},"source":["- Augment training dataset and re-train your model"]},{"cell_type":"markdown","metadata":{"id":"7t5UZmogV1Hf"},"source":["- First step is to create the augmented version of the dataset "]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":194274,"status":"ok","timestamp":1680882340120,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"_lh-1eseV1Hf","outputId":"051fdddf-501e-43d4-d5c8-163bf9a42ed0"},"outputs":[{"name":"stderr","output_type":"stream","text":["98176it [03:12, 511.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Initial samples: 98176\n","Augmented samples: 196352\n","Augmented train dataset path: /content/drive/MyDrive/epfl/modernNLP/a2-MChatzakis/A2/nli_data/train_augmented_simple.jsonl\n","Augmentation methods used in pairs:\n","{('random_deletion', 'random_deletion'): 6073,\n"," ('random_deletion', 'random_insertion'): 6191,\n"," ('random_deletion', 'swap_word'): 6181,\n"," ('random_deletion', 'synonym_replacement'): 6299,\n"," ('random_insertion', 'random_deletion'): 6094,\n"," ('random_insertion', 'random_insertion'): 6080,\n"," ('random_insertion', 'swap_word'): 6019,\n"," ('random_insertion', 'synonym_replacement'): 5961,\n"," ('swap_word', 'random_deletion'): 6121,\n"," ('swap_word', 'random_insertion'): 6331,\n"," ('swap_word', 'swap_word'): 6094,\n"," ('swap_word', 'synonym_replacement'): 6199,\n"," ('synonym_replacement', 'random_deletion'): 6341,\n"," ('synonym_replacement', 'random_insertion'): 6007,\n"," ('synonym_replacement', 'swap_word'): 6182,\n"," ('synonym_replacement', 'synonym_replacement'): 6003}\n"]}],"source":["# augmentation methods\n","def simple_augmentation(sample, n, p):\n","    \"\"\"Takes a sample of the dataset and augments it.\n","       The augmentation is performed by randomly selecting two methods\n","       to augment each sentence of the sample\n","\n","    Args:\n","        sample (_type_): sample\n","        n (int): n\n","        p (float): deletion prob p\n","\n","    Returns:\n","        tuple: (list of new samples, method used)\n","    \"\"\"\n","    aug_methods = [synonym_replacement, random_deletion, swap_word, random_insertion]\n","\n","    method1 = aug_methods[random.randint(0, len(aug_methods) - 1)]\n","    method2 = aug_methods[random.randint(0, len(aug_methods) - 1)]\n","\n","    new_sample = sample.copy()\n","\n","    if method1 == random_deletion:\n","        new_sample[\"premise\"] = random_deletion(new_sample[\"premise\"], p, n)\n","    elif method1 == swap_word:\n","        new_sample[\"premise\"] = swap_word(new_sample[\"premise\"])\n","    else:\n","        new_sample[\"premise\"] = method1(new_sample[\"premise\"], n)\n","\n","    if method2 == random_deletion:\n","        new_sample[\"hypothesis\"] = random_deletion(new_sample[\"hypothesis\"], p, n)\n","    elif method2 == swap_word:\n","        new_sample[\"hypothesis\"] = swap_word(new_sample[\"hypothesis\"])\n","    else:\n","        new_sample[\"hypothesis\"] = method2(new_sample[\"hypothesis\"], n)\n","\n","    return [sample, new_sample], (method1.__name__, method2.__name__)\n","\n","\n","def augment_nli(input_data_path, output_data_path, n, p):\n","    \"\"\"Augment NLI dataset\n","\n","    Args:\n","        input_data_path (string): path to NLI dataset\n","        output_data_path (string): path to save the augmented dataset\n","        n (int): n\n","        p (float): p\n","    \"\"\"\n","    samples = []\n","    initial_samples = 0\n","    methods_used_count = {}\n","    with jsonlines.open(input_data_path, \"r\") as reader:\n","        for sample in tqdm(reader.iter()):\n","            aug_sample_list, used_methods = simple_augmentation(sample, n, p)\n","\n","            samples += aug_sample_list\n","            initial_samples += 1\n","\n","            if used_methods in methods_used_count:\n","                methods_used_count[used_methods] = methods_used_count[used_methods] + 1\n","            else:\n","                methods_used_count[used_methods] = 1\n","\n","    # Write samples to new dataset\n","    with jsonlines.open(output_data_path, \"w\") as writer:\n","        writer.write_all(samples)\n","\n","    print(f\"Initial samples: {initial_samples}\")\n","    print(f\"Augmented samples: {len(samples)}\")\n","    print(f\"Augmented train dataset path: {output_data_path}\")\n","    print(f\"Augmentation methods used in pairs:\")\n","    pprint.pprint(methods_used_count)\n","\n","\n","# Create augmented dataset\n","train_data_path = ROOT_PATH + \"/nli_data/train.jsonl\"\n","augmented_data_path = ROOT_PATH + \"/nli_data/train_augmented_simple.jsonl\"\n","p = 0.2\n","n = 2\n","augment_nli(train_data_path, augmented_data_path, n, p)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"f-Tt__mtV1Hf"},"source":["- Now, that the augmented dataset is available under nli_data, we retrain the model with the new version of the dataset and we save it under runs/augmentation."]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5083355,"status":"ok","timestamp":1680887423472,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"OWNstkmsV1Hf","outputId":"162a0f5c-5615-415b-e786-63527ffd5f11"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["196352it [02:51, 1141.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["9815it [00:07, 1331.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training model augmentation lr=2e-05\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 12272/12272 [26:57<00:00,  7.59it/s]\n","Evaluation: 100%|██████████| 614/614 [00:23<00:00, 26.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Training Loss: 0.682 | Validation Loss: 0.586\n","Epoch 0 NLI Validation:\n","Accuracy: 77.31% | F1: (80.46%, 73.47%, 77.81%) | Macro-F1: 77.25%\n","Model Saved!\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 12272/12272 [26:45<00:00,  7.64it/s]\n","Evaluation: 100%|██████████| 614/614 [00:23<00:00, 26.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | Training Loss: 0.359 | Validation Loss: 0.747\n","Epoch 1 NLI Validation:\n","Accuracy: 76.45% | F1: (79.40%, 72.62%, 77.34%) | Macro-F1: 76.45%\n","Model Saved!\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 12272/12272 [26:44<00:00,  7.65it/s]\n","Evaluation: 100%|██████████| 614/614 [00:23<00:00, 26.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 | Training Loss: 0.164 | Validation Loss: 1.218\n","Epoch 2 NLI Validation:\n","Accuracy: 76.29% | F1: (79.49%, 71.69%, 77.22%) | Macro-F1: 76.13%\n","Model Saved!\n"]}],"source":["# Retrain based on the augmented dataset\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","model.to(device)\n","\n","train_dataset = NLIDataset(augmented_data_path, tokenizer)\n","dev_dataset = NLIDataset(ROOT_PATH + \"/nli_data/dev_in_domain.jsonl\", tokenizer)\n","\n","batch_size = 16\n","epochs = 3\n","max_grad_norm = 1.0\n","warmup_percent = 0.3\n","model_save_root = ROOT_PATH + \"/runs/augmentation/\"\n","lr = 2e-5\n","\n","print(f\"Training model augmentation lr={lr}\")\n","train(\n","    train_dataset,\n","    dev_dataset,\n","    model,\n","    device,\n","    batch_size,\n","    epochs,\n","    lr,\n","    warmup_percent,\n","    max_grad_norm,\n","    model_save_root,\n",")"]},{"cell_type":"markdown","metadata":{"id":"os0S3iGhV1Hf"},"source":["- Load the saved model to begin evaluation"]},{"cell_type":"markdown","metadata":{"id":"L7QWE_eYV1Hg"},"source":["- Them we check again the performance on the by-domain test sets. Note that we use the initial test sets we produced in task 1, to be able to compare the performance of the model in both cases."]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40167,"status":"ok","timestamp":1680887511043,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"vHrZIqq9V1Hg","outputId":"ebd96dfb-aaac-475e-f2ed-d9a857aefaea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["1973it [00:01, 1191.73it/s]\n","Evaluation: 100%|██████████| 124/124 [00:03<00:00, 34.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: fiction\n","Validation Loss: 1.251 | Accuracy: 75.16%\n","F1: (77.47%, 70.76%, 76.97%) | Macro-F1: 75.06%\n","Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["1945it [00:04, 389.65it/s]\n","Evaluation: 100%|██████████| 122/122 [00:04<00:00, 25.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: government\n","Validation Loss: 0.968 | Accuracy: 81.23%\n","F1: (83.53%, 77.90%, 81.94%) | Macro-F1: 81.13%\n","Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["1955it [00:03, 581.32it/s]\n","Evaluation: 100%|██████████| 123/123 [00:04<00:00, 26.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: slate\n","Validation Loss: 1.493 | Accuracy: 70.74%\n","F1: (73.83%, 67.03%, 71.13%) | Macro-F1: 70.66%\n","Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["1966it [00:01, 1452.09it/s]\n","Evaluation: 100%|██████████| 123/123 [00:05<00:00, 21.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: telephone\n","Validation Loss: 1.223 | Accuracy: 75.74%\n","F1: (79.60%, 68.67%, 77.76%) | Macro-F1: 75.35%\n","Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["1976it [00:02, 706.25it/s]\n","Evaluation: 100%|██████████| 124/124 [00:04<00:00, 25.98it/s]"]},{"name":"stdout","output_type":"stream","text":["Domain: travel\n","Validation Loss: 1.131 | Accuracy: 78.64%\n","F1: (82.25%, 74.66%, 78.54%) | Macro-F1: 78.48%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["batch_size = 16\n","learning_rate = 2e-5\n","warmup_percent = 0.3\n","checkpoint = ROOT_PATH + \"/runs/augmentation/lr{}-warmup{}\".format(\n","    learning_rate, warmup_percent\n",")\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n","model = DistilBertForSequenceClassification.from_pretrained(checkpoint)\n","model.to(device)\n","\n","\n","# validation on the old data, using initial test sets\n","for domain in [\"fiction\", \"government\", \"slate\", \"telephone\", \"travel\"]:\n","    # Evaluate and save prediction results in each domain\n","    filename = ROOT_PATH + \"/nli_data/\" + \"dev_in_domain\" + \"_\" + domain + \".jsonl\"\n","    dev_domain_dataset = NLIDataset(filename, tokenizer)\n","\n","    dev_loss, acc, f1_ent, f1_neu, f1_con = evaluate(\n","        dev_domain_dataset,\n","        model,\n","        device,\n","        batch_size,\n","        no_labels=False,\n","        result_save_file=ROOT_PATH\n","        + \"/predictions/augmentation/\"\n","        + \"predictions\"\n","        + \"_\"\n","        + domain\n","        + \".jsonl\",\n","    )\n","    macro_f1 = (f1_ent + f1_neu + f1_con) / 3\n","\n","    print(f\"Domain: {domain}\")\n","    print(f\"Validation Loss: {dev_loss:.3f} | Accuracy: {acc*100:.2f}%\")\n","    print(\n","        f\"F1: ({f1_ent*100:.2f}%, {f1_neu*100:.2f}%, {f1_con*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%\"\n","    )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Discussion for performance of augmentation on by-domain sets**\n","\n","By re-run the model on the by-domain validation tests we see that the performance is close to the one we got without augmentation.\n","\n","In addition, we see that the performance (accuracy and f1-score) slightly improved, and went up to 81% on both metrics.\n","\n","Overall, we see that for the by-domain validation sets the performance is approximately the same with what we had without augmenting the data. One reason why we do not see better performance could be that the augmentation adds samples on the dataset that do not represent the actual cases that are encountered during testing, thus they do not offer the additional samples we would want to show to the model during training.\n"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1680887512626,"user":{"displayName":"Manos Chatzakis","userId":"03353778924019204355"},"user_tz":-180},"id":"WUFzQqoeV1Hg","outputId":"aab64de1-f34c-48ee-b0a9-47285a6d62b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Building NLI Dataset...\n"]},{"name":"stderr","output_type":"stream","text":["100it [00:00, 1321.91it/s]\n","Evaluation: 100%|██████████| 7/7 [00:00<00:00, 29.23it/s]"]},{"name":"stdout","output_type":"stream","text":["Annotated data testing finished.\n","Accuracy: 0.64\n","Macro-f1: 0.6367521286010742\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# test on new annotated data\n","filename = ROOT_PATH + \"/nli_data/test_data/05-final.jsonl\"\n","dev_domain_dataset = NLIDataset(filename, tokenizer)\n","dev_loss, acc, f1_ent, f1_neu, f1_con = evaluate(\n","    dev_domain_dataset,\n","    model,\n","    device,\n","    batch_size,\n","    no_labels=False,\n","    result_save_file=ROOT_PATH + \"/predictions/augmentation/predictions_final_annotations.jsonl\",\n",")\n","macro_f1 = (f1_ent + f1_neu + f1_con) / 3\n","\n","print(\"Annotated data testing finished.\")\n","print(f\"Accuracy: {acc}\")\n","print(f\"Macro-f1: {macro_f1}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Discussion for performance of augmentation on new test data**\n","\n","As we see from the previous cell, the performance of the model with the augmented dataset does not improve with the annotated data we created, and the overall accuracy and macro-f1 score is slightly less than the one we got when we trained with the initial data.\n","\n","An explanation of this behavior is that:\n","* The augmentation of the dataset includes a bit of noise in the training set (as we discussed in the forum), leading to scenarios that the model's performance does not necessarily improve.\n","* The augmentation is based on the initial training data. This means that augmented samples are more likely to improve the model performance on the by-domain validation sets we have seen, and not on unseen test data. This could explain the reason we still see good performance on the by-domain validation testing and not on the unseen annotated data."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**General comments on augmentation**\n","\n","In orded to maintain a small training time and managable dataset size, we were limited to the augmentation method and training presented above.\n","\n","Perhaps, if we had a more capable system, a model with higher degree of augmentation and more hyperparameter tuning."]},{"cell_type":"markdown","metadata":{"id":"M3CIeN_kaOQl"},"source":["### **5 Upload Your Notebook, Data and Models**\n","\n","Please **rename** your filled jupyter notebook as **your Sciper number** and upload it to your GitHub Classroom repository, **with all cells run and output results shown**.\n","\n","**Note:** We are **not** responsible for re-running the cells in your notebook.\n","\n","Please also submit all your processed (e.g., anotated and augmented) datasets, as well as all your trained models in Task 1 and Task 4, in your GitHub Classroom repository.\n","\n","The datasets and models that you need to submit include:\n","\n","**1. The best model checkpoint you trained in the Section 1.2 \"Start Training and Validation!\"**\n","\n","**2. The best model prediction results in the Section 1.2 \"Fine-Grained Validation\"**\n","\n","**3. Your annotated test dataset in the Section 3.2 \"Annotate Your 100 Datapoints with Partner(s)\"**\n","\n","**4. Your augmented training data and best model checkpoint in the Section 4.2 \"Augment Your Model\"**\n","\n","**Note:** You may need to use [GitHub LFS](https://edstem.org/eu/courses/379/discussion/27240) for submitting large files."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["1. Best checkpoints for both model (provided both of them): \n","    * runs/lr2e-05-warmup0.3\n","    * runs/lr5e-05-warmup0.3\n","2. Best model prediction results for each domain:\n","    * predictions/predictions_{domain}.txt (.txt does not matter as an extension)\n","3. Annotated data:\n","    * My annotations: /nli_data/test_data/05-353068.jsonl (student_label_2)\n","    * Final annotations: /nli_data/test_data/05-final.json\n","4. Model and data:\n","    * Model: runs/augmentation/lr2e-05-warmup0.3\n","    * Data: nli_data/train_augmented_simple.jsonl\n","\n","Please do not hesitate to contact me in case something is corrupted or is problematic."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["9MSpYuMcyHfl"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
